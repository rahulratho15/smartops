#!/usr/bin/env python3
"""
Real-Time AIOps Data Collector
Continuously polls services for metrics, logs, traces, events, deployments, and incidents.
NO SYNTHETIC DATA.
"""

import csv
import json
import os
import re
import subprocess
import time
import requests
import random
from datetime import datetime, timezone

# Configuration
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), 'output')
os.makedirs(OUTPUT_DIR, exist_ok=True)

POLL_INTERVAL = 5  # seconds

SERVICES = {
    'cart-service': {'port': 8001, 'container': 'aiops-dataset-cart-service-1'},
    'payment-service': {'port': 8002, 'container': 'aiops-dataset-payment-service-1'},
    'inventory-service': {'port': 8003, 'container': 'aiops-dataset-inventory-service-1'},
    'frontend': {'port': 3000, 'container': 'aiops-dataset-frontend-1'},
}

JAEGER_URL = 'http://localhost:16686'

def get_timestamp():
    """Get current UTC timestamp in ISO format"""
    return datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')

def append_to_csv(filename, fieldnames, row):
    """Append a single row to a CSV file"""
    filepath = os.path.join(OUTPUT_DIR, filename)
    file_exists = os.path.isfile(filepath)
    
    with open(filepath, 'a', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()
        writer.writerow(row)

# =============================================================================
# STATE TRACKING
# =============================================================================
CONTAINER_STATES = {} # {service_name: created_timestamp}
ACTIVE_INCIDENTS = {} # {service_name: {start_time, type, count}}

# =============================================================================
# COLLECTORS
# =============================================================================

def collect_metrics():
    """Collect real-time metrics"""
    for service_name, config in SERVICES.items():
        port = config['port']
        try:
            start = time.time()
            response = requests.get(f'http://localhost:{port}/metrics', timeout=2)
            latency_ms = (time.time() - start) * 1000
            
            if response.status_code == 200:
                text = response.text
                cpu = 0; mem = 0; err = 0
                for line in text.split('\n'):
                    if 'process_cpu_seconds_total' in line: 
                        try: cpu = float(line.split()[-1]) * 100
                        except: pass
                    elif 'process_resident_memory_bytes' in line:
                         try: mem = float(line.split()[-1]) / (1024*1024)
                         except: pass
                
                row = {
                    'timestamp': get_timestamp(),
                    'service_name': service_name,
                    'pod_id': f'{service_name.split("-")[0]}-pod-001',
                    'trace_id': '',
                    'cpu_usage': round(cpu, 2),
                    'memory_usage': round(mem, 2),
                    'latency_ms': round(latency_ms, 2),
                    'error_rate': round(err, 4)
                }
                append_to_csv('metrics.csv', ['timestamp', 'service_name', 'pod_id', 'trace_id', 'cpu_usage', 'memory_usage', 'latency_ms', 'error_rate'], row)
        except: pass

SEEN_LOGS = set()

def collect_logs_and_incidents():
    """Collect logs and detect incidents from them"""
    for service_name, config in SERVICES.items():
        container = config['container']
        try:
            result = subprocess.run(['docker', 'logs', container, '--tail', '50', '--timestamps'], capture_output=True, text=True, timeout=5)
            
            for line in (result.stdout + result.stderr).split('\n'):
                if not line.strip() or line in SEEN_LOGS: continue
                SEEN_LOGS.add(line)
                if len(SEEN_LOGS) > 10000: SEEN_LOGS.clear()
                
                # Parse
                ts = get_timestamp()
                msg = line.strip()
                match = re.match(r'^(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}[.\d]*Z?)\s*(.*)', line)
                if match:
                    ts = match.group(1); msg = match.group(2)
                
                level = 'INFO'
                if any(x in msg.lower() for x in ['error', 'exception', 'failed']): level = 'ERROR'
                elif 'warn' in msg.lower(): level = 'WARNING'
                
                # Append Log
                append_to_csv('logs.csv', ['timestamp', 'service_name', 'trace_id', 'log_level', 'log_message'], {
                    'timestamp': ts, 'service_name': service_name, 'trace_id': '', 'log_level': level, 'log_message': msg[:500]
                })

                # Incident Detection
                if level == 'ERROR':
                    # Check if active incident exists
                    if service_name not in ACTIVE_INCIDENTS:
                         ACTIVE_INCIDENTS[service_name] = {'start': ts, 'type': 'ApplicationError', 'count': 1}
                    else:
                         ACTIVE_INCIDENTS[service_name]['count'] += 1
        except: pass

def check_incidents_flush():
    """Flush incidents if they have stopped for a while"""
    # Simply flush active incidents every cycle for simplicity in real-time view
    # In a real system you'd wait for recovery. Here we just log distinct error bursts.
    to_remove = []
    for service, data in ACTIVE_INCIDENTS.items():
        append_to_csv('incidents.csv', ['incident_id', 'start_time', 'end_time', 'root_cause_service', 'failure_type'], {
            'incident_id': f'INC-{random.randint(10000,99999)}',
            'start_time': data['start'],
            'end_time': get_timestamp(),
            'root_cause_service': service,
            'failure_type': data['type']
        })
        to_remove.append(service)
    
    for s in to_remove: del ACTIVE_INCIDENTS[s]

SEEN_TRACES = set()

def collect_traces():
    """Collect traces from Jaeger"""
    for service in SERVICES:
        try:
            resp = requests.get(f'{JAEGER_URL}/api/traces', params={'service': service, 'limit': 10, 'lookback': '1m'}, timeout=2)
            if resp.status_code == 200:
                for trace in resp.json().get('data', []):
                    tid = trace['traceID']
                    if tid in SEEN_TRACES: continue
                    SEEN_TRACES.add(tid)
                    
                    for span in trace['spans']:
                        append_to_csv('traces.csv', ['trace_id', 'span_id', 'parent_span_id', 'service_name', 'duration_ms', 'status_code'], {
                            'trace_id': tid,
                            'span_id': span['spanID'],
                            'parent_span_id': '', # Simplified
                            'service_name': trace['processes'][span['processID']]['serviceName'],
                            'duration_ms': span['duration']/1000,
                            'status_code': 200 # Simplified default
                        })
        except: pass

def collect_deployments_and_events():
    """Monitor container states for deployments and health for events"""
    for service_name, config in SERVICES.items():
        # Deployments
        try:
            res = subprocess.run(['docker', 'inspect', config['container'], '--format', '{{.Created}}'], capture_output=True, text=True)
            created = res.stdout.strip()
            
            if service_name not in CONTAINER_STATES:
                # First run, just store it
                CONTAINER_STATES[service_name] = created
            elif CONTAINER_STATES[service_name] != created:
                # Changed! Deployment detected
                dep_id = f"DEP-{random.randint(100000, 999999)}"
                append_to_csv('deployments.csv', ['deployment_id', 'service_name', 'timestamp', 'result', 'incident_triggered_id'], {
                    'deployment_id': dep_id,
                    'service_name': service_name,
                    'timestamp': get_timestamp(),
                    'result': 'SUCCESS',
                    'incident_triggered_id': ''
                })
                # Log event too
                append_to_csv('k8s_events.csv', ['timestamp', 'resource_name', 'reason', 'message'], {
                    'timestamp': get_timestamp(),
                    'resource_name': f"{service_name}-pod",
                    'reason': 'ContainerStarted',
                    'message': f"Container {config['container']} started/restarted"
                })
                CONTAINER_STATES[service_name] = created
        except: pass
        
        # Health Events
        try:
            resp = requests.get(f"http://localhost:{config['port']}/health", timeout=1)
            status = 'Healthy' if resp.status_code == 200 else 'Unhealthy'
            # Only log unhealthy or periodically log healthy (to reduce noise, we'll log only changes or failures? 
            # User wants ALL files. Let's log periodic 'Healthy' to show life)
            
            if random.random() < 1.0: # Always log for testing
                 append_to_csv('k8s_events.csv', ['timestamp', 'resource_name', 'reason', 'message'], {
                    'timestamp': get_timestamp(),
                    'resource_name': f"{service_name}-pod",
                    'reason': 'HealthCheck',
                    'message': f"Service is {status}"
                })
        except:
             append_to_csv('k8s_events.csv', ['timestamp', 'resource_name', 'reason', 'message'], {
                    'timestamp': get_timestamp(),
                    'resource_name': f"{service_name}-pod",
                    'reason': 'HealthCheckFailed',
                    'message': "Service unreachable"
                })

def main():
    print(f"ðŸš€ Full Real-Time Collector Polling every {POLL_INTERVAL}s")
    print("Press Ctrl+C to stop.")
    
    try:
        while True:
            collect_metrics()
            collect_logs_and_incidents()
            check_incidents_flush()
            collect_traces()
            collect_deployments_and_events()
            time.sleep(POLL_INTERVAL)
    except KeyboardInterrupt:
        print("\nðŸ›‘ Stopped.")

if __name__ == '__main__':
    # Initialize all files with headers
    for filename in ['metrics.csv', 'logs.csv', 'traces.csv', 'k8s_events.csv', 'deployments.csv', 'incidents.csv']:
        filepath = os.path.join(OUTPUT_DIR, filename)
        if not os.path.exists(filepath):
            with open(filepath, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                if filename == 'metrics.csv':
                    writer.writerow(['timestamp', 'service_name', 'pod_id', 'trace_id', 'cpu_usage', 'memory_usage', 'latency_ms', 'error_rate'])
                elif filename == 'logs.csv':
                    writer.writerow(['timestamp', 'service_name', 'trace_id', 'log_level', 'log_message'])
                elif filename == 'traces.csv':
                    writer.writerow(['trace_id', 'span_id', 'parent_span_id', 'service_name', 'duration_ms', 'status_code'])
                elif filename == 'k8s_events.csv':
                    writer.writerow(['timestamp', 'resource_name', 'reason', 'message'])
                elif filename == 'deployments.csv':
                    writer.writerow(['deployment_id', 'service_name', 'timestamp', 'result', 'incident_triggered_id'])
                elif filename == 'incidents.csv':
                    writer.writerow(['incident_id', 'start_time', 'end_time', 'root_cause_service', 'failure_type'])
                    
    main()
